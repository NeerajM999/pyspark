{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn pySpark using jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([('Ross', 19), ('Joey', 18), ('Rachel', 17)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ross', 19), ('Joey', 18), ('Rachel', 17)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file:///Users/Neeraj/Documents/Proj/Spark/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"some_sql_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    "  .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myrange = spark.range(10,30).toDF(\"number\")\n",
    "myrange.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divby2 = myrange.where(\"number%2 = 0\")\n",
    "divby2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row((number + 1)=11),\n",
       " Row((number + 1)=12),\n",
       " Row((number + 1)=13),\n",
       " Row((number + 1)=14),\n",
       " Row((number + 1)=15),\n",
       " Row((number + 1)=16),\n",
       " Row((number + 1)=17),\n",
       " Row((number + 1)=18),\n",
       " Row((number + 1)=19),\n",
       " Row((number + 1)=20),\n",
       " Row((number + 1)=21),\n",
       " Row((number + 1)=22),\n",
       " Row((number + 1)=23),\n",
       " Row((number + 1)=24),\n",
       " Row((number + 1)=25),\n",
       " Row((number + 1)=26),\n",
       " Row((number + 1)=27),\n",
       " Row((number + 1)=28),\n",
       " Row((number + 1)=29),\n",
       " Row((number + 1)=30)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#myrange.show(5)\n",
    "myrange.select(myrange[\"number\"] + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a json file into dataframe\n",
    "df = spark.read.format(\"json\").load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "#note that it will infer the schema automatically for json\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access same file from hdfs\n",
    "df = spark.read.format(\"json\").load(\"/data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets peek few records\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can print schema of a dataframe as\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get schema as structure\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dest: string (nullable = true)\n",
      " |-- orig: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+----+----+-----+\n",
      "|dest|orig|count|\n",
      "+----+----+-----+\n",
      "|null|null|   15|\n",
      "|null|null|    1|\n",
      "|null|null|  344|\n",
      "+----+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create custom schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "mySchema = StructType(\n",
    "    [\n",
    "        StructField(\"dest\", StringType(), True),\n",
    "        StructField(\"orig\", StringType(), True),\n",
    "        StructField(\"count\", LongType(), False, metadata={\"type\":\"airports-count\"})\n",
    "    ]\n",
    ")\n",
    "\n",
    "#use this schema while importing data\n",
    "df = spark.read.format(\"json\").schema(mySchema).load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "df.schema\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4eb8d4b66442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'column'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "df.columns\n",
    "df.column(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'DEST_COUNTRY_NAME'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"count\"]\n",
    "df.DEST_COUNTRY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fname: string (nullable = false)\n",
      " |-- Lname: string (nullable = true)\n",
      " |-- age: long (nullable = false)\n",
      "\n",
      "+-----+-----+---+\n",
      "|Fname|Lname|age|\n",
      "+-----+-----+---+\n",
      "|Peter| null| 30|\n",
      "| John|Smith| 20|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "myschema = StructType([\n",
    "    StructField(\"Fname\", StringType(), False),\n",
    "    StructField(\"Lname\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), False)\n",
    "])\n",
    "\n",
    "myRow1 = Row(\"Peter\",None,30)\n",
    "myRow2 = Row(\"John\", \"Smith\", 20)\n",
    "\n",
    "# create a df from row\n",
    "mydf = spark.createDataFrame([myRow1, myRow2], myschema)\n",
    "mydf.printSchema()\n",
    "mydf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp table\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select expressions\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"DEST_COUNTRY_NAME\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"DEST_COUNTRY_NAME\"]).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         dest|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as dest\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|new_count|\n",
      "+---------+\n",
      "|       30|\n",
      "|        2|\n",
      "+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"count * 2  as new_count\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|count|evenOrodd|\n",
      "+-----+---------+\n",
      "|   15|    false|\n",
      "|    1|    false|\n",
      "|  344|     true|\n",
      "|   15|    false|\n",
      "|   62|     true|\n",
      "|    1|    false|\n",
      "|   62|     true|\n",
      "|  588|     true|\n",
      "|   40|     true|\n",
      "|    1|    false|\n",
      "+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"count\"), expr(\"count % 2 = 0 as evenOrodd\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|count|isEven|\n",
      "+-----+------+\n",
      "|   15| false|\n",
      "|    1| false|\n",
      "|  344|  true|\n",
      "|   15| false|\n",
      "|   62|  true|\n",
      "|    1| false|\n",
      "|   62|  true|\n",
      "|  588|  true|\n",
      "|   40|  true|\n",
      "|    1| false|\n",
      "+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"count\"), expr(\"count %2 = 0\").alias(\"isEven\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|count|isEven|\n",
      "+-----+------+\n",
      "|   15| false|\n",
      "|    1| false|\n",
      "|  344|  true|\n",
      "|   15| false|\n",
      "|   62|  true|\n",
      "|    1| false|\n",
      "|   62|  true|\n",
      "|  588|  true|\n",
      "|   40|  true|\n",
      "|    1| false|\n",
      "+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"count\", \"count%2=0 as isEven\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+------+-----------------------------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|isEven|(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME)|\n",
      "+-----------------+-------------------+-----+------+-----------------------------------------+\n",
      "|    United States|            Romania|   15| false|                                    false|\n",
      "|    United States|            Croatia|    1| false|                                    false|\n",
      "|    United States|            Ireland|  344|  true|                                    false|\n",
      "|            Egypt|      United States|   15| false|                                    false|\n",
      "|    United States|              India|   62|  true|                                    false|\n",
      "+-----------------+-------------------+-----+------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\", \"count\", \"count%2=0 as isEven\", \"DEST_COUNTRY_NAME=ORIGIN_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "|    United States|              India|   62|        false|\n",
      "|    United States|          Singapore|    1|        false|\n",
      "|    United States|            Grenada|   62|        false|\n",
      "|       Costa Rica|      United States|  588|        false|\n",
      "|          Senegal|      United States|   40|        false|\n",
      "|          Moldova|      United States|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME=ORIGIN_COUNTRY_NAME) as withinCountry\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------------+\n",
      "|round(avg(count), 2)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+--------------------+---------------------------------+\n",
      "|             1770.77|                              132|\n",
      "+--------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run custom functions\n",
    "df.selectExpr(\"round(avg(count),2)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "|    United States|            Ireland|  344|  1|\n",
      "|            Egypt|      United States|   15|  1|\n",
      "|    United States|              India|   62|  1|\n",
      "|    United States|          Singapore|    1|  1|\n",
      "|    United States|            Grenada|   62|  1|\n",
      "|       Costa Rica|      United States|  588|  1|\n",
      "|          Senegal|      United States|   40|  1|\n",
      "|          Moldova|      United States|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add new column\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.withColumn(\"One\", lit(1)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "|    United States|              India|   62|        false|\n",
      "|    United States|          Singapore|    1|        false|\n",
      "|    United States|            Grenada|   62|        false|\n",
      "|       Costa Rica|      United States|  588|        false|\n",
      "|          Senegal|      United States|   40|        false|\n",
      "|          Moldova|      United States|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename a column or add new column\n",
    "df.withColumn(\"withinCountry\", expr(\"DEST_COUNTRY_NAME==ORIGIN_COUNTRY_NAME\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|         dest|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "|United States|            Ireland|  344|\n",
      "|        Egypt|      United States|   15|\n",
      "|United States|              India|   62|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename column\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|    United States|   15|\n",
      "|    United States|    1|\n",
      "|    United States|  344|\n",
      "+-----------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove column\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- new_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# type cast a column\n",
    "df.withColumn(\"new_count\", col(\"count\").cast(\"int\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- new-count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new-count\", expr(\"count\").cast(\"int\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "|    United States|           Malaysia|    3|\n",
      "|         Thailand|      United States|    3|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the data\n",
    "df.where(\"count < 10\").show(3)\n",
    "\n",
    "# or \n",
    "df.filter((col(\"count\") > 2) & (col(\"count\")<5)).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "+-----------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same orig and dest\n",
    "df.where(\"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"count < 2\").where(\"ORIGIN_COUNTRY_NAME != 'Croatia'\").show(5)\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using SQL\n",
    "spark.sql(\"\"\"\n",
    "    select * from dfTable where count < 2 and origin_country_name != 'Croatia'\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find distinct records from a dataframe\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|count(DISTINCT named_struct(DEST_COUNTRY_NAME, DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, ORIGIN_COUNTRY_NAME))|\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                         256|\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sql\n",
    "spark.sql(\"select count(distinct(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME)) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|         Anguilla|      United States|   41|\n",
      "|          Algeria|      United States|    4|\n",
      "|            Italy|      United States|  382|\n",
      "|      The Bahamas|      United States|  955|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get random samples from a dataframe\n",
    "from random import randint\n",
    "df.sample(False, 0.1, randint(0,100)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint],\n",
       " DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split a dataframe into 2 dataframes in specific proportions\n",
    "from random import uniform\n",
    "x = round(uniform(0.1, 1.0),2)\n",
    "print(x)\n",
    "df.randomSplit([x, 1-x], randint(0,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|               c1|                 c2|   10|\n",
      "|               c1|                 c3|   12|\n",
      "|               c2|                 c1|    5|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union of 2 dataframes.\n",
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "rows = [Row(\"c1\", \"c2\", 10), Row(\"c1\", \"c3\", 12), Row(\"c2\", \"c1\", 5)]\n",
    "\n",
    "newDF = spark.createDataFrame(spark.sparkContext.parallelize(rows), schema)\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Egypt|      United States|   15|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "|           Guyana|      United States|   64|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now union it with df \n",
    "df.union(newDF).where(\"DEST_COUNTRY_NAME != 'United States'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort rows in a dataframe\n",
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|      United States|348113|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Canada|  8305|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|      United States|358354|\n",
      "|    United States|      United States|352742|\n",
      "|    United States|      United States|348113|\n",
      "|    United States|      United States|347452|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Malta|      United States|    1|\n",
      "|            Yemen|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort in desc order\n",
    "from pyspark.sql.functions import asc, desc\n",
    "df.orderBy(col(\"count\").desc()).show(5)\n",
    "\n",
    "#or\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(5)\n",
    "\n",
    "#using expr\n",
    "df.orderBy(expr(\"count desc\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sorting with nulls in a dataframe\n",
    "from pyspark.sql.functions import asc_nulls_first, asc_nulls_last, desc_nulls_first, desc_nulls_last\n",
    "df.orderBy(col(\"count\").asc_nulls_first()).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|      United States|348113|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Canada|  8305|\n",
      "|           Canada|      United States|  8271|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "|    United States|             Mexico|  6220|\n",
      "|           Mexico|      United States|  6200|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for performance it's better to sort within partitions first before doing any transformations. \n",
    "# This can be done at the time of loading\n",
    "df = spark.read.format(\"json\") \\\n",
    "    .load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/flight-data/json/*-summary.json\") \\\n",
    "    .sortWithinPartitions(col(\"count\").desc())\n",
    "\n",
    "df.count()\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition - always cause shuffling of data across partitions. It is better to use when target partitions are more\n",
    "# than current no. of partitions or you would like to partition by a column if filter logic depends on a col.\n",
    "df.rdd.getNumPartitions() # 3\n",
    "\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\")).rdd.getNumPartitions() # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also specify no. of partitions\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce - combines partitions without full shuffling\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(3).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x1138ed668>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect entire dataframe - dangerous\n",
    "df.collect()\n",
    "\n",
    "# better option is to use iterator which will still be dangerous if application reading data sequentially doesn't\n",
    "# manage memory footprint intelligently\n",
    "# toLocalIterator - consumes as much memory as the largest partition in the DF. If partition is large, it may still \n",
    "# crash the driver program\n",
    "\n",
    "df.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|      United States|348113|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Canada|  8305|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[370004, 348115, 8485, 8401, 8307]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map function on a dataframe\n",
    "df.select(\"*\").show(5)\n",
    "l = df.select(\"count\").rdd.map(lambda x: x[0] + 2).take(5)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load CSV file and handle different data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True)\\\n",
    "    .load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/by-day/2010-12-01.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|InvoiceNo|StockCode|\n",
      "+---------+---------+\n",
      "|536366   |22633    |\n",
      "|536366   |22632    |\n",
      "|536367   |84879    |\n",
      "|536367   |22745    |\n",
      "|536367   |22748    |\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# boolean conditions or equalto \n",
    "from pyspark.sql.functions import col\n",
    "df.where(\"invoiceno != 536365\").select(\"InvoiceNo\", \"StockCode\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'(instr(Description, POSTAGE) >= 1)'>\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|UnitPrice|stockcode|\n",
      "+---------+---------+\n",
      "|   569.77|      DOT|\n",
      "|   607.49|      DOT|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or we can do it this way\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descFilter))\\\n",
    "    .where(\"isExpensive\")\\\n",
    "    .select(\"UnitPrice\", \"stockcode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n",
      "|   536545|    21134|       null|       1|2010-12-01 14:32:00|      0.0|      null|United Kingdom|\n",
      "|   536546|    22145|       null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n",
      "|   536547|    37509|       null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n",
      "|   536549|   85226A|       null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n",
      "|   536550|    85044|       null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n",
      "|   536552|    20950|       null|       1|2010-12-01 14:34:00|      0.0|      null|United Kingdom|\n",
      "|   536553|    37461|       null|       3|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n",
      "|   536554|    84670|       null|      23|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n",
      "|   536589|    21777|       null|     -10|2010-12-01 16:50:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in case of data having nulls, you need to take care of it\n",
    "df.where(\"Description is null\").show()\n",
    "\n",
    "# make it null safe\n",
    "df.where(col(\"Description\").eqNullSafe(\"foo\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------------------+--------------------+\n",
      "|stockcode|quantity|unitprice|round(new_price, 0)|bround(new_price, 0)|\n",
      "+---------+--------+---------+-------------------+--------------------+\n",
      "|   85123A|       6|     2.55|              239.0|               239.0|\n",
      "|    71053|       6|     3.39|              419.0|               419.0|\n",
      "|   84406B|       8|     2.75|              489.0|               489.0|\n",
      "|   84029G|       6|     3.39|              419.0|               419.0|\n",
      "|   84029E|       6|     3.39|              419.0|               419.0|\n",
      "+---------+--------+---------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mathemathical computations - pow\n",
    "\n",
    "from pyspark.sql.functions import pow, round, bround\n",
    "\n",
    "new_price = pow(df.Quantity * df.UnitPrice,2) + 5\n",
    "\n",
    "df.withColumn(\"new_price\", new_price)\\\n",
    "    .select(\"stockcode\", \"quantity\", \"unitprice\", round(\"new_price\",0), bround(\"new_price\",0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|         UnitPrice|          Quantity|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              3108|              3108|\n",
      "|   mean| 4.151946589446603| 8.627413127413128|\n",
      "| stddev|15.638659854603892|26.371821677029203|\n",
      "|    min|               0.0|               -24|\n",
      "|    max|            607.49|               600|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "+-------------+\n",
      "|min(Quantity)|\n",
      "+-------------+\n",
      "|          -24|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get common statistics about a dataframe\n",
    "df.describe(\"UnitPrice\", \"Quantity\").show()\n",
    "df.selectExpr(\"min(Quantity)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|initcap(Description)|  upper(Description)|  lower(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|White Hanging Hea...|WHITE HANGING HEA...|white hanging hea...|\n",
      "| White Metal Lantern| WHITE METAL LANTERN| white metal lantern|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# working with Strings\n",
    "from pyspark.sql.functions import initcap, upper, lower\n",
    "\n",
    "df.select(initcap(col(\"Description\")), upper(col(\"Description\")), lower(col(\"Description\"))).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+---+----------+\n",
      "|    ltrim|    rtrim| trim| lp|        rp|\n",
      "+---------+---------+-----+---+----------+\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "+---------+---------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lpad, ltrim, rtrim, rpad, trim\n",
    "\n",
    "\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"    HELLO    \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"    HELLO    \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \"x\").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \"x\").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|KNITTED UNION FLA...|\n",
      "|COLOR WOOLLY HOTT...|RED WOOLLY HOTTIE...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regex replace\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_pattern = \"WHITE|BLACK|RED|GREEN|BLUE\"\n",
    "replace_with = \"COLOR\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_pattern, \"COLOR\").alias(\"color_clean\"), col(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|color|         Description|\n",
      "+-----+--------------------+\n",
      "|WHITE|WHITE HANGING HEA...|\n",
      "|WHITE| WHITE METAL LANTERN|\n",
      "|     |CREAM CUPID HEART...|\n",
      "|     |KNITTED UNION FLA...|\n",
      "|  RED|RED WOOLLY HOTTIE...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regex extract\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "pattern = \"(WHITE|BLACK|GREEN|RED|BLUE)\"\n",
    "df.select(regexp_extract(col(\"Description\"),pattern,1).alias(\"color\"), \"Description\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------+\n",
      "|description                       |white_black|\n",
      "+----------------------------------+-----------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|true       |\n",
      "|WHITE METAL LANTERN               |true       |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |true       |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|true       |\n",
      "|WHITE METAL LANTERN               |true       |\n",
      "+----------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check existence of a substring\n",
    "contains_black = instr(col(\"Description\"), \"BLACK\") >=1\n",
    "contains_white = instr(col(\"Description\"), \"WHITE\") >=1\n",
    "df.withColumn(\"white_black\", contains_black | contains_white)\\\n",
    "    .where(\"white_black\").select(\"description\",\"white_black\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+------+--------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|is_black|is_white|is_blue|is_red|is_green|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+--------+--------+-------+------+--------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|false   |true    |false  |false |false   |536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|false   |true    |false  |false |false   |536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|false   |true    |false  |true  |false   |536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.    |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+--------+--------+-------+------+--------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "simpleColors = ['black', 'white', 'blue', 'red', 'green']\n",
    "\n",
    "def color_locator(column, color):\n",
    "    return locate(color.upper(), column).cast(\"boolean\").alias(\"is_\"+color)\n",
    "\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\"))\n",
    "\n",
    "df.select(selectedColumns).where(expr(\"is_white OR is_red\")).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coalesce - get first non-null value from a list of columns in a df\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------+------------+\n",
      "|      ifnull|nullif|         nvl|        nvl2|\n",
      "+------------+------+------------+------------+\n",
      "|return-value|  null|return-value|return-value|\n",
      "+------------+------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select ifnull(null, 'return-value') as `ifnull`,\n",
    "        nullif('value', 'value') as `nullif`,\n",
    "        nvl(null, 'return-value') as `nvl`,\n",
    "        nvl2('non-null','return-value', 'else-value') as `nvl2`\n",
    "    from dfTable limit 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total= 3108\n",
      "Drop= 1968\n",
      "Drop any= 1968\n",
      "Drop all= 3108\n"
     ]
    }
   ],
   "source": [
    "print(\"Total=\", df.count())\n",
    "# drop rows with nulls\n",
    "print(\"Drop=\", df.na.drop().count())\n",
    "\n",
    "# drop if any of the cols is null\n",
    "print(\"Drop any=\", df.na.drop('any').count())\n",
    "\n",
    "# drop if all of the cols is null\n",
    "print(\"Drop all=\", df.na.drop('all').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any =  3098\n",
      "all =  3108\n"
     ]
    }
   ],
   "source": [
    "# drop row when certain cols are null\n",
    "print(\"any = \", df.na.drop(\"any\", subset = [\"StockCode\", \"InvoiceNo\", \"Description\"]).count())\n",
    "print(\"all = \", df.na.drop(\"all\", subset = [\"StockCode\", \"InvoiceNo\", \"Description\"]).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill default value for any null col in DF\n",
    "df.na.fill(\"Replace NULL with this\")\n",
    "\n",
    "# do it for specific cols\n",
    "df.na.fill(\"New String\", subset=[\"StockCode\", \"Description\"] )\n",
    "\n",
    "# different values for cols\n",
    "df.na.fill({\"StockCode\":5, \"Description\":\"empty\", \"InvoiceNo\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "df.na.replace(\"WHITE\", \"SPACE\", subset=[\"Description\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|         complex|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+----------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|[536365, 85123A]|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "| [536365, 71053]|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+----------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# structure\n",
    "df.selectExpr(\"struct(InvoiceNo, StockCode) as complex\", \"*\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+--------------------+\n",
      "| complex.Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "\n",
    "# access cols using dot notation\n",
    "complexDF.select(\"complex.Description\").show(4)\n",
    "\n",
    "# or getField function\n",
    "complexDF.select(col(\"complex\").getField(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|complex[0]|\n",
      "+----------+\n",
      "|WHITE     |\n",
      "|WHITE     |\n",
      "|CREAM     |\n",
      "|KNITTED   |\n",
      "|RED       |\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array\n",
    "# convert a col into array of values\n",
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"complex\")).selectExpr(\"complex[0]\").show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------------------------+\n",
      "|Description                        |size(split(Description,  ))|\n",
      "+-----------------------------------+---------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |5                          |\n",
      "|WHITE METAL LANTERN                |3                          |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |5                          |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|6                          |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |5                          |\n",
      "+-----------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#size of array\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(\"Description\", size(split(col(\"Description\"), \" \"))).show(5, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------------------------------------------+\n",
      "|Description                        |array_contains(split(Description,  ), WHITE)|\n",
      "+-----------------------------------+--------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |true                                        |\n",
      "|WHITE METAL LANTERN                |true                                        |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |false                                       |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|false                                       |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |true                                        |\n",
      "+-----------------------------------+--------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array contains a value\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(\"Description\", array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------------+-----+\n",
      "|Description                        |splitted_col                              |exploded_col|rowid|\n",
      "+-----------------------------------+------------------------------------------+------------+-----+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |WHITE       |0    |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |HANGING     |1    |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |HEART       |2    |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |T-LIGHT     |3    |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |HOLDER      |4    |\n",
      "|WHITE METAL LANTERN                |[WHITE, METAL, LANTERN]                   |WHITE       |5    |\n",
      "|WHITE METAL LANTERN                |[WHITE, METAL, LANTERN]                   |METAL       |6    |\n",
      "|WHITE METAL LANTERN                |[WHITE, METAL, LANTERN]                   |LANTERN     |7    |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[CREAM, CUPID, HEARTS, COAT, HANGER]      |CREAM       |8    |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[CREAM, CUPID, HEARTS, COAT, HANGER]      |CUPID       |9    |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[CREAM, CUPID, HEARTS, COAT, HANGER]      |HEARTS      |10   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[CREAM, CUPID, HEARTS, COAT, HANGER]      |COAT        |11   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[CREAM, CUPID, HEARTS, COAT, HANGER]      |HANGER      |12   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|KNITTED     |13   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|UNION       |14   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|FLAG        |15   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|HOT         |16   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|WATER       |17   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|BOTTLE      |18   |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[RED, WOOLLY, HOTTIE, WHITE, HEART.]      |RED         |19   |\n",
      "+-----------------------------------+------------------------------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode each word into a new row while keeping rest of the cols same\n",
    "from pyspark.sql.functions import split, explode, monotonically_increasing_id, posexplode\n",
    "\n",
    "df.withColumn(\"splitted_col\", split(col(\"Description\"), \" \"))\\\n",
    "    .withColumn(\"exploded_col\", explode(col(\"splitted_col\")))\\\n",
    "    .withColumn(\"rowid\", monotonically_increasing_id()) \\\n",
    "    .select(\"Description\", \"splitted_col\", \"exploded_col\", \"rowid\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|map(InvoiceNo, Description)                    |\n",
      "+-----------------------------------------------+\n",
      "|[536365 -> WHITE HANGING HEART T-LIGHT HOLDER] |\n",
      "|[536365 -> WHITE METAL LANTERN]                |\n",
      "|[536365 -> CREAM CUPID HEARTS COAT HANGER]     |\n",
      "|[536365 -> KNITTED UNION FLAG HOT WATER BOTTLE]|\n",
      "|[536365 -> RED WOOLLY HOTTIE WHITE HEART.]     |\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create maps\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "df.select(create_map(col(\"InvoiceNo\"), col(\"Description\"))).show(5, False)\n",
    "\n",
    "# or\n",
    "#df.select(map(col(\"InvoiceNo\"), col(\"Description\")).alias(\"complex\")).selectExpr(\"complex[536365]\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# udf\n",
    "udfDF = spark.range(5).toDF(\"num\")\n",
    "udfDF.createOrReplaceTempView(\"udfDF_table\")\n",
    "udfDF.show()\n",
    "\n",
    "#define udf function\n",
    "def power3(val):\n",
    "    return val**3\n",
    "\n",
    "# test udf\n",
    "power3(2)\n",
    "\n",
    "# register the function as udf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "power3udf = udf(power3)\n",
    "spark.udf.register(\"power3udf\", power3, IntegerType())\n",
    "\n",
    "# use it like a normal function on dataframe\n",
    "udfDF.select(power3udf(col(\"num\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|power3udf(num)|\n",
      "+--------------+\n",
      "|             0|\n",
      "|             1|\n",
      "|             8|\n",
      "|            27|\n",
      "|            64|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also use python registered udf function in spark sql\n",
    "spark.sql(\"\"\"select power3udf(num) from udfDF_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", True).option(\"inferSchema\", True)\\\n",
    "    .load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/all/*.csv\")\\\n",
    "    .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, countDistinct\n",
    "df.select(count(\"StockCode\")).show()\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from dfTable\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get first and last row from DF\n",
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get min and max value from DF\n",
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n",
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get sum of all values in a column\n",
    "from pyspark.sql.functions import sum, sumDistinct\n",
    "df.select(sum(\"Quantity\")).show()\n",
    "\n",
    "# sum of distinct values - heavy operation\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------------+----------------+\n",
      "|(Total_Purchases / Total_Quantities)|Average_Quantity|   Mean_Quantity|\n",
      "+------------------------------------+----------------+----------------+\n",
      "|                    9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate average of 2 columns\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(count(\"Quantity\").alias(\"Total_Quantities\"),\\\n",
    "         sum(\"Quantity\").alias(\"Total_Purchases\"),\\\n",
    "         avg(\"Quantity\").alias(\"Average_Quantity\"),\\\n",
    "         expr(\"mean(Quantity)\").alias(\"Mean_Quantity\"))\\\n",
    "    .selectExpr(\"Total_Purchases/Total_Quantities\", \\\n",
    "                \"Average_Quantity\",\\\n",
    "                \"Mean_Quantity\"\\\n",
    "               ).show()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660923| 47559.39140929892|  218.08095663447835|   218.08115785023455|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standard deviation and variance\n",
    "from pyspark.sql.functions import var_pop, stddev_pop, var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610528376|119768.05495530753|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# skewness and kurtosis\n",
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     4.912186085640497E-4|            1052.7260778754955|             1052.7280543915997|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# covariance and correlation\n",
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\"),covar_samp(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gather values in a list or set (unique)\n",
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   552340|     17949|    1|\n",
      "|   567695|      null|    1|\n",
      "|  C562144|     12757|    1|\n",
      "|   550531|     13199|    1|\n",
      "|   551692|      null|    1|\n",
      "|  C543757|     13115|    1|\n",
      "|   552215|      null|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   562463|      null|    1|\n",
      "|  C544318|     12989|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping is a 2 step process - specify group columns and then aggregate function\n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().orderBy(col(\"count\").asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------+--------------+--------------+\n",
      "|InvoiceNo|count(Quantity)|sum(Quantity)|max(UnitPrice)|min(UnitPrice)|\n",
      "+---------+---------------+-------------+--------------+--------------+\n",
      "|   536596|              6|            9|         19.95|          0.29|\n",
      "|   536938|             14|          464|         10.95|          0.85|\n",
      "|   537252|              1|           31|          0.85|          0.85|\n",
      "|   537691|             20|          163|          9.95|          0.65|\n",
      "|   538041|              1|           30|           0.0|           0.0|\n",
      "+---------+---------------+-------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(count(\"Quantity\"),expr(\"sum(Quantity)\"), expr(\"max(UnitPrice)\"), min(col(\"UnitPrice\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# window range and rowsbetween\n",
    "\n",
    "Creates a WindowSpec with the frame boundaries defined, from start (inclusive) to end (inclusive).\n",
    "\n",
    "Both start and end are relative positions from the current row. For example, “0” means “current row”, while “-1” means the row before the current row, and “5” means the fifth row after the current row.\n",
    "\n",
    "We recommend users use Window.unboundedPreceding, Window.unboundedFollowing, and Window.currentRow to specify special boundary values, rather than using integral values directly.\n",
    "\n",
    "Parameters:\t\n",
    "start – boundary start, inclusive. The frame is unbounded if this is Window.unboundedPreceding, or any value less than or equal to -9223372036854775808.\n",
    "end – boundary end, inclusive. The frame is unbounded if this is Window.unboundedFollowing, or any value greater than or equal to 9223372036854775807."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next create window specification\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "WindowSpec = Window\\\n",
    "    .partitionBy(\"CustomerID\", \"date\")\\\n",
    "    .orderBy(desc(\"Quantity\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the aggregations\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(WindowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ranks\n",
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(WindowSpec)\n",
    "purchaseRank = rank().over(WindowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+--------------------+\n",
      "|CustomerID|      date|Quantity|PurchaseRank|PurchaseDenseRank|Max PurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+--------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|               74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|               74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                  36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                  36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                  36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                  36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                  36|\n",
      "+----------+----------+--------+------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerID is not null\").orderBy(\"CustomerID\")\\\n",
    "    .select(\"CustomerID\", \"date\", \"Quantity\", \\\n",
    "            purchaseRank.alias(\"PurchaseRank\"), purchaseDenseRank.alias(\"PurchaseDenseRank\"), \\\n",
    "            maxPurchaseQuantity.alias(\"Max PurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-----------+\n",
      "|CustomerId|      date|Quantity|PurchaseRank|PurchaseDenseRank|maxQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-----------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|      74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|      74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|         36|\n",
      "|     12347|2010-12-07|      30|           2|                2|         36|\n",
      "|     12347|2010-12-07|      24|           3|                3|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|      12|           4|                4|         36|\n",
      "|     12347|2010-12-07|       6|          17|                5|         36|\n",
      "|     12347|2010-12-07|       6|          17|                5|         36|\n",
      "+----------+----------+--------+------------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same using sql\n",
    "spark.sql(\"\"\"\n",
    "    select CustomerId, date, Quantity,\n",
    "    rank(Quantity) over (partition by CustomerId, date order by Quantity desc nulls last ROWS between unbounded preceding and current row) as PurchaseRank,\n",
    "    dense_rank(Quantity) over (partition by customerID, date order by quantity desc nulls last ROWS between unbounded preceding and current row) as PurchaseDenseRank,\n",
    "    max(Quantity) over (partition by customerId, date order by quantity desc nulls last rows between unbounded preceding and current row) as maxQuantity\n",
    "    from dfWithDate\n",
    "    where customerId is not null\n",
    "    order by customerId, date, quantity desc \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|customerId|stockcode|sum(quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "|     18287|    84584|            6|\n",
      "|     18287|   84507C|            6|\n",
      "|     18287|   72351B|           24|\n",
      "|     18287|   72351A|           24|\n",
      "|     18287|   72349B|           60|\n",
      "|     18287|    47422|           24|\n",
      "|     18287|    47421|           48|\n",
      "|     18287|    35967|           36|\n",
      "|     18287|    23445|           20|\n",
      "|     18287|    23378|           24|\n",
      "|     18287|    23376|           48|\n",
      "|     18287|    23310|           36|\n",
      "|     18287|    23274|           12|\n",
      "|     18287|    23272|           12|\n",
      "|     18287|    23269|           36|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find sum of quantities sold per stockcode and customer\n",
    "# using grouping sets which must not have null values in the input data\n",
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
    "\n",
    "gset = spark.sql(\"\"\"\n",
    "    select customerid, stockcode, sum(quantity) from dfNoNull\n",
    "    group by grouping sets((customerId, stockcode), ())\n",
    "    order by customerId desc, stockcode desc\n",
    "\"\"\")\n",
    "gset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       5176450|\n",
      "|2010-12-01|   null|         26814|\n",
      "|2010-12-02|   null|         21023|\n",
      "|2010-12-03|   null|         14830|\n",
      "|2010-12-05|   null|         16395|\n",
      "|2010-12-06|   null|         21419|\n",
      "|2010-12-07|   null|         24995|\n",
      "|2010-12-08|   null|         22741|\n",
      "|2010-12-09|   null|         18431|\n",
      "|2010-12-10|   null|         20297|\n",
      "|2010-12-12|   null|         10565|\n",
      "|2010-12-13|   null|         17623|\n",
      "|2010-12-14|   null|         20098|\n",
      "|2010-12-15|   null|         18229|\n",
      "|2010-12-16|   null|         29632|\n",
      "|2010-12-17|   null|         16069|\n",
      "|2010-12-19|   null|          3795|\n",
      "|2010-12-20|   null|         14965|\n",
      "|2010-12-21|   null|         15467|\n",
      "|2010-12-22|   null|          3192|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RollUP - Hierarchical grouping\n",
    "rolledDF = dfNoNull.rollup(\"date\", \"Country\").agg(sum(\"Quantity\").alias(\"total_quantity\"))\\\n",
    "    .selectExpr(\"Date\", \"Country\", \"total_quantity\")\\\n",
    "    .orderBy(\"Date\")\n",
    "\n",
    "rolledDF.where(\"country is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----+\n",
      "|Date|           Country|Total|\n",
      "+----+------------------+-----+\n",
      "|null|           Denmark| 8188|\n",
      "|null|European Community|  497|\n",
      "|null|            Norway|19247|\n",
      "+----+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-------+-------+\n",
      "|      Date|Country|  Total|\n",
      "+----------+-------+-------+\n",
      "|      null|   null|5176450|\n",
      "|2010-12-01|   null|  26814|\n",
      "|2010-12-02|   null|  21023|\n",
      "+----------+-------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cube - runs aggregation over all combinations of grouped fields. It can answer following questions:\n",
    "# The total across all dates and countries\n",
    "# The total for each date across all countries\n",
    "# The total for each country on each date\n",
    "# The total for each country across all dates\n",
    "\n",
    "cubeDF = dfNoNull.cube(\"Date\", \"Country\").agg(sum(\"Quantity\").alias(\"Total\"))\\\n",
    "    .select(\"Date\", \"Country\", \"Total\")\\\n",
    "    .orderBy(\"Date\")\n",
    "\n",
    "cubeDF.show(3)\n",
    "cubeDF.where(\"country is null\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+-------+\n",
      "|      Date|Country|gid|  total|\n",
      "+----------+-------+---+-------+\n",
      "|      null|   null|  3|5176450|\n",
      "|2011-06-01|   null|  1|  10880|\n",
      "|2011-07-26|   null|  1|  13560|\n",
      "|2011-07-04|   null|  1|  15682|\n",
      "|2011-05-25|   null|  1|  11839|\n",
      "|2011-08-01|   null|  1|   9947|\n",
      "|2011-03-04|   null|  1|  13332|\n",
      "|2011-05-18|   null|  1|  18785|\n",
      "|2011-07-22|   null|  1|  10019|\n",
      "|2011-07-27|   null|  1|  13859|\n",
      "|2011-06-22|   null|  1|  15769|\n",
      "|2011-06-27|   null|  1|   9835|\n",
      "|2011-10-19|   null|  1|  19039|\n",
      "|2011-01-21|   null|  1|  14938|\n",
      "|2011-09-22|   null|  1|  32263|\n",
      "|2011-01-09|   null|  1|   8181|\n",
      "|2011-01-20|   null|  1|   8720|\n",
      "|2011-09-18|   null|  1|   8966|\n",
      "|2011-08-15|   null|  1|  10141|\n",
      "|2011-01-24|   null|  1|  11910|\n",
      "+----------+-------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping ids to differentiate between aggregation levels 0...3\n",
    "from pyspark.sql.functions import grouping_id, desc\n",
    "\n",
    "rolledDF = dfNoNull.rollup(\"Date\", \"Country\").agg(grouping_id().alias(\"gid\"), sum(\"Quantity\").alias('total'))\\\n",
    "    .orderBy(expr(\"gid\").desc())\n",
    "rolledDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+---+-------+\n",
      "|Date|           Country|gid|  total|\n",
      "+----+------------------+---+-------+\n",
      "|null|              null|  3|5176450|\n",
      "|null|             Spain|  2|  26824|\n",
      "|null|European Community|  2|    497|\n",
      "+----+------------------+---+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-------+---+-----+\n",
      "|      Date|Country|gid|total|\n",
      "+----------+-------+---+-----+\n",
      "|2011-01-20|   null|  1| 8720|\n",
      "|2011-01-21|   null|  1|14938|\n",
      "+----------+-------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+--------------+---+-----+\n",
      "|      Date|       Country|gid|total|\n",
      "+----------+--------------+---+-----+\n",
      "|2011-02-09|       Germany|  0|  160|\n",
      "|2011-02-13|United Kingdom|  0| 2715|\n",
      "+----------+--------------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cube with grouping_id\n",
    "# Grouping ids to differentiate between aggregation levels 0...3\n",
    "from pyspark.sql.functions import grouping_id, desc\n",
    "\n",
    "cubeDF = dfNoNull.cube(\"Date\", \"Country\").agg(grouping_id().alias(\"gid\"), sum(\"Quantity\").alias('total'))\\\n",
    "    .orderBy(expr(\"gid\").desc())\n",
    "cubeDF.show(3)\n",
    "\n",
    "cubeDF.where(\"gid =1\").show(2)\n",
    "cubeDF.where(\"gid =0\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+------------------+\n",
      "|      date|USA_sum(CAST(Quantity AS BIGINT))|USA_sum(UnitPrice)|\n",
      "+----------+---------------------------------+------------------+\n",
      "|2011-12-06|                             null|              null|\n",
      "|2011-12-09|                             null|              null|\n",
      "|2011-12-08|                             -196|             13.75|\n",
      "|2011-12-07|                             null|              null|\n",
      "+----------+---------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot - rows to columns\n",
    "\n",
    "pivoted = dfNoNull.groupBy(\"Date\").pivot(\"Country\").sum()\n",
    "pivoted.where(\"date > '2011-12-05'\")\\\n",
    "    .select(\"date\", \"`USA_sum(CAST(Quantity AS BIGINT))`\",\\\n",
    "            \"`USA_sum(UnitPrice)`\"\n",
    "           )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inner joins (keep rows with keys that exist in the left and right datasets)\n",
      "\n",
      "Outer joins (keep rows with keys in either the left or right datasets)\n",
      "\n",
      "Left outer joins (keep rows with keys in the left dataset)\n",
      "\n",
      "Right outer joins (keep rows with keys in the right dataset)\n",
      "\n",
      "Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n",
      "\n",
      "Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\n",
      "\n",
      "Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n",
      "\n",
      "Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark supports various types of joins: \n",
    "print(\"\"\"\n",
    "Inner joins (keep rows with keys that exist in the left and right datasets)\n",
    "\n",
    "Outer joins (keep rows with keys in either the left or right datasets)\n",
    "\n",
    "Left outer joins (keep rows with keys in the left dataset)\n",
    "\n",
    "Right outer joins (keep rows with keys in the right dataset)\n",
    "\n",
    "Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n",
    "\n",
    "Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\n",
    "\n",
    "Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n",
    "\n",
    "Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes for joins \n",
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "  .toDF(\"id\", \"status\")\n",
    "\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join (default)\n",
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or \n",
    "person.join(graduateProgram, joinExpression, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.join(person, joinExpression, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# outer join\n",
    "person.join(graduateProgram, joinExpression, \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or\n",
    "graduateProgram.join(person, joinExpression, \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left outer join\n",
    "person.join(graduateProgram, joinExpression, \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you switch the tables, you will get extra rows\n",
    "graduateProgram.join(person, joinExpression, \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# right outer join\n",
    "person.join(graduateProgram, joinExpression, \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left semi join - only matching rows from left and only left table cols. If left table has duplicates, \n",
    "# you will still get them. It is mor like a filter which shows only matching rows in left and right.\n",
    "graduateProgram.join(person, joinExpression, \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left table has duplicate rows, so you will still get it if it's matching with right table\n",
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "\n",
    "gradProgram2.join(person, joinExpression, \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left anti joins - shows only rows which are not matching with right table. Only gives left columns in output\n",
    "\n",
    "graduateProgram.join(person, joinExpression, \"left_anti\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|graduate_program| id|            name|   spark_status| degree|          department|     school|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|               0|  0|   Bill Chambers|          [100]|Masters|School of Informa...|UC Berkeley|\n",
      "|               1|  1|   Matei Zaharia|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "|               1|  2|Michael Armbrust|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# natual joins - implictly joins tables based on same cols names which could be dangerous. Avoid this join\n",
    "spark.sql(\" select * from graduateProgram natural join person\").show()\n",
    "\n",
    "# this result is wrong, because person.id has different meaning from graduateProgram.id.\n",
    "\n",
    "# or, rename the column and then join like this. Note 'common col' is dropped as well automatically\n",
    "gradProg = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "person.join(gradProg, \"graduate_program\").select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** This result is not cross join due to joinExpression\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n",
      "*** You can explicity run cross join:\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross join - cartesian product. By default its disabled. You can enable it: set spark.sql.crossJoin.enabled=true\n",
    "print(\"**** This result is not cross join due to joinExpression\")\n",
    "person.join(graduateProgram, joinExpression, \"cross\").show() # \n",
    "\n",
    "print(\"*** You can explicity run cross join:\")\n",
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personid|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# complex joins\n",
    "from pyspark.sql.functions import expr\n",
    "pdf = person.withColumnRenamed(\"id\", \"personid\")\\\n",
    "    .join(sparkStatus, expr(\"array_contains(spark_status, id)\"))\n",
    "\n",
    "pdf.show()\n",
    "\n",
    "# person.spark_status and sparkStatus.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+------------+\n",
      "|personid|            name|graduate_program|   spark_status| id|        status|       rowid|\n",
      "+--------+----------------+----------------+---------------+---+--------------+------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor| 60129542144|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President| 77309411328|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member| 85899345920|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor| 94489280512|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|120259084288|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|128849018880|\n",
      "+--------+----------------+----------------+---------------+---+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cobmine all values of status in same order\n",
    "p = pdf.withColumn(\"rowid\", monotonically_increasing_id())\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+\n",
      "|          rd|position|sparkid|\n",
      "+------------+--------+-------+\n",
      "| 60129542144|       0|    100|\n",
      "| 77309411328|       0|    500|\n",
      "| 77309411328|       1|    250|\n",
      "| 77309411328|       2|    100|\n",
      "| 85899345920|       0|    500|\n",
      "| 85899345920|       1|    250|\n",
      "| 85899345920|       2|    100|\n",
      "| 94489280512|       0|    500|\n",
      "| 94489280512|       1|    250|\n",
      "| 94489280512|       2|    100|\n",
      "|120259084288|       0|    250|\n",
      "|120259084288|       1|    100|\n",
      "|128849018880|       0|    250|\n",
      "|128849018880|       1|    100|\n",
      "+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import posexplode\n",
    "\n",
    "pp = p.select(\"rowid\", posexplode(\"spark_status\"))\\\n",
    "    .withColumnRenamed(\"rowid\", 'rd')\\\n",
    "    .withColumnRenamed(\"pos\", \"position\")\\\n",
    "    .withColumnRenamed(\"col\", \"sparkid\")\n",
    "\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+---+--------------+\n",
      "|          rd|position|sparkid| id|        status|\n",
      "+------------+--------+-------+---+--------------+\n",
      "| 77309411328|       1|    250|250|    PMC Member|\n",
      "| 85899345920|       1|    250|250|    PMC Member|\n",
      "| 94489280512|       1|    250|250|    PMC Member|\n",
      "|120259084288|       0|    250|250|    PMC Member|\n",
      "|128849018880|       0|    250|250|    PMC Member|\n",
      "| 77309411328|       0|    500|500|Vice President|\n",
      "| 85899345920|       0|    500|500|Vice President|\n",
      "| 94489280512|       0|    500|500|Vice President|\n",
      "| 60129542144|       0|    100|100|   Contributor|\n",
      "| 77309411328|       2|    100|100|   Contributor|\n",
      "| 85899345920|       2|    100|100|   Contributor|\n",
      "| 94489280512|       2|    100|100|   Contributor|\n",
      "|120259084288|       1|    100|100|   Contributor|\n",
      "|128849018880|       1|    100|100|   Contributor|\n",
      "+------------+--------+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pc = pp.join(sparkStatus, expr(\"sparkid == id\"))\n",
    "pc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+---+--------------+-----------------------------------------+\n",
      "|rd          |position|sparkid|id |status        |sparks                                   |\n",
      "+------------+--------+-------+---+--------------+-----------------------------------------+\n",
      "|128849018880|0       |250    |250|PMC Member    |[PMC Member, Contributor]                |\n",
      "|128849018880|1       |100    |100|Contributor   |[PMC Member, Contributor]                |\n",
      "|60129542144 |0       |100    |100|Contributor   |[Contributor]                            |\n",
      "|85899345920 |0       |500    |500|Vice President|[Vice President, PMC Member, Contributor]|\n",
      "|85899345920 |1       |250    |250|PMC Member    |[Vice President, PMC Member, Contributor]|\n",
      "|85899345920 |2       |100    |100|Contributor   |[Vice President, PMC Member, Contributor]|\n",
      "|120259084288|0       |250    |250|PMC Member    |[PMC Member, Contributor]                |\n",
      "|120259084288|1       |100    |100|Contributor   |[PMC Member, Contributor]                |\n",
      "|94489280512 |0       |500    |500|Vice President|[Vice President, PMC Member, Contributor]|\n",
      "|94489280512 |1       |250    |250|PMC Member    |[Vice President, PMC Member, Contributor]|\n",
      "|94489280512 |2       |100    |100|Contributor   |[Vice President, PMC Member, Contributor]|\n",
      "|77309411328 |0       |500    |500|Vice President|[Vice President, PMC Member, Contributor]|\n",
      "|77309411328 |1       |250    |250|PMC Member    |[Vice President, PMC Member, Contributor]|\n",
      "|77309411328 |2       |100    |100|Contributor   |[Vice President, PMC Member, Contributor]|\n",
      "+------------+--------+-------+---+--------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcc = pc.withColumn(\"sparks\", collect_list(\"status\")\\\n",
    "    .over(Window\\\n",
    "          .partitionBy(\"rd\")\\\n",
    "          .orderBy(\"position\")\\\n",
    "          .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)))\\\n",
    "    .selectExpr(\"*\")\n",
    "\n",
    "pcc.show(truncate=False)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+-----------------------------------------+\n",
      "|personid|name            |graduate_program|spark_status   |sparks                                   |\n",
      "+--------+----------------+----------------+---------------+-----------------------------------------+\n",
      "|0       |Bill Chambers   |0               |[100]          |[Contributor]                            |\n",
      "|1       |Matei Zaharia   |1               |[500, 250, 100]|[Vice President, PMC Member, Contributor]|\n",
      "|2       |Michael Armbrust|1               |[250, 100]     |[PMC Member, Contributor]                |\n",
      "+--------+----------------+----------------+---------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1 = p.join(pcc, expr(\"rd == rowid\"))\\\n",
    "    .drop(\"rd\")\\\n",
    "    .drop(\"rowid\")\\\n",
    "    .drop(\"id\")\\\n",
    "    .drop(\"position\")\\\n",
    "    .drop(\"sparkid\", \"status\", \"spark\")\\\n",
    "    .dropDuplicates()\\\n",
    "    .orderBy(\"personid\")\n",
    "\n",
    "p1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [graduate_program#130360L], [id#130374L], Inner\n",
      ":- *(2) Sort [graduate_program#130360L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(graduate_program#130360L, 200)\n",
      ":     +- *(1) Project [_1#130350L AS id#130358L, _2#130351 AS name#130359, _3#130352L AS graduate_program#130360L, _4#130353 AS spark_status#130361]\n",
      ":        +- *(1) Filter isnotnull(_3#130352L)\n",
      ":           +- Scan ExistingRDD[_1#130350L,_2#130351,_3#130352L,_4#130353]\n",
      "+- *(4) Sort [id#130374L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(id#130374L, 200)\n",
      "      +- *(3) Project [_1#130366L AS id#130374L, _2#130367 AS degree#130375, _3#130368 AS department#130376, _4#130369 AS school#130377]\n",
      "         +- *(3) Filter isnotnull(_1#130366L)\n",
      "            +- Scan ExistingRDD[_1#130366L,_2#130367,_3#130368,_4#130369]\n"
     ]
    }
   ],
   "source": [
    "# check the explain plan for our joins\n",
    "person.join(graduateProgram, joinExpression, \"inner\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [graduate_program#130360L], [id#130374L], Inner, BuildRight\n",
      ":- *(2) Project [_1#130350L AS id#130358L, _2#130351 AS name#130359, _3#130352L AS graduate_program#130360L, _4#130353 AS spark_status#130361]\n",
      ":  +- *(2) Filter isnotnull(_3#130352L)\n",
      ":     +- Scan ExistingRDD[_1#130350L,_2#130351,_3#130352L,_4#130353]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "   +- *(1) Project [_1#130366L AS id#130374L, _2#130367 AS degree#130375, _3#130368 AS department#130376, _4#130369 AS school#130377]\n",
      "      +- *(1) Filter isnotnull(_1#130366L)\n",
      "         +- Scan ExistingRDD[_1#130366L,_2#130367,_3#130368,_4#130369]\n"
     ]
    }
   ],
   "source": [
    "# we can explictly push broadcast join like this\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "person.join(broadcast(graduateProgram), joinExpression).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read from csv files\n",
    "# mode = permissive, failFast, dropMalformed\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"header\",True)\\\n",
    "    .load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into CSV file\n",
    "# mode - overwrite, append, errorIfExists, ignore\n",
    "# maxRecordsPerFile allows controlling output file sizes\n",
    "df.write.format(\"csv\").option(\"mode\", \"overwrite\")\\\n",
    "    .option(\"dateFormat\", \"MM-dd-yyyy\")\\\n",
    "    .option(\"sep\", \"\\t\")\\\n",
    "    .option(\"maxRecordsPerFile\", 50)\\\n",
    "    .save(\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/output/2010-12-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also partition DF before writing out.\n",
    "# sort and bucketBy aren't supported as of spark 2.4 release\n",
    "df.write.format(\"csv\").option(\"mode\", \"overwrite\").option(\"sep\", \"\\t\")\\\n",
    "    .partitionBy(\"Country\")\\\n",
    "    .save(\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/output/2010-12-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading JSON files\n",
    "df = spark.read.format(\"json\").option(\"mode\", \"failfast\")\\\n",
    "    .load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/flight-data/json\")\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv to json data\n",
    "df.write.format(\"json\").option(\"mode\", \"overwrite\")\\\n",
    "    .save(\"file:///Users/Neeraj/Documents/Proj/Spark/data/flight-data/json/2010-retail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, partitioned column will be removed from the data records\n",
    "df.write.format(\"json\").option(\"mode\", \"overwrite\")\\\n",
    "    .partitionBy(\"Country\")\\\n",
    "    .save(\"file:///Users/Neeraj/Documents/Proj/Spark/data/flight-data/json/2011-retail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parquet use format(\"parquet\")\n",
    "# for ORC use format(\"orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to PostgresSQL or any Database DB\n",
    "Start Jupyter as:\n",
    "pyspark --driver-class-path postgresql-42.2.5.jar --jars postgresql-42.2.5.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way is to pass in driver class path with postgres jar while starting pyspark in terminal\n",
    "# bin\\pyspark --driver-class-path $PATH_TO_DRIVER_JAR --jars $PATH_TO_DRIVER_JAR\n",
    "# e.g pyspark --driver-class-path postgresql-42.2.5.jar --jars postgresql-42.2.5.jar\n",
    "\n",
    "# OR, with spark-shell, do this\n",
    "# .\\bin\\spark-shell --packages org.postgresql:postgresql:42.1.1\n",
    "# The driver file will automatically be downloaded if needed into spark’s jars folder\n",
    "# read more: https://medium.com/@thucnc/pyspark-in-jupyter-notebook-working-with-dataframe-jdbc-data-sources-6f3d39300bf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to do it in jupyter, then follow this\n",
    "import os\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:jar:42.2.5 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///Users/Neeraj/Documents/Proj/Spark/pyspark/postgresql-42.2.5.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now prepare JDBC URI and dictionary of arguments\n",
    "url = \"jdbc:postgresql://127.0.0.1/course_data\"\n",
    "driver = \"org.postgresql.Driver\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"abc123\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+----------+----------------+------+------+---------+\n",
      "|employee_id|first_name|last_name|               email| hire_date|      department|gender|salary|region_id|\n",
      "+-----------+----------+---------+--------------------+----------+----------------+------+------+---------+\n",
      "|          1|    Berrie|  Manueau|bmanueau0@dion.ne.jp|2006-04-20|          Sports|     F|154864|        4|\n",
      "|          2|   Aeriell|    McNee|   amcnee1@google.es|2009-01-26|           Tools|     F| 56752|        3|\n",
      "|          3|    Sydney|  Symonds|   ssymonds2@hhs.gov|2010-05-17|        Clothing|     F| 95313|        4|\n",
      "|          4|     Avrom|Rowantree|                null|2014-08-02|Phones & Tablets|     M|119674|        7|\n",
      "|          5|    Feliks|  Morffew|    fmorffew4@a8.net|2003-01-14|       Computers|     M| 55307|        5|\n",
      "|          6|   Bethena|     Trow|btrow5@technorati...|2003-06-08|          Sports|     F|134501|        3|\n",
      "|          7|    Ardeen|  Curwood|  acurwood6@1und1.de|2006-02-19|        Clothing|     F| 28995|        7|\n",
      "|          8|    Seline|   Dubber|sdubber7@t-online.de|2012-05-28|Phones & Tablets|     F|101066|        3|\n",
      "|          9|     Dayle|    Trail|    dtrail8@tamu.edu|2003-03-01|       First Aid|     F| 82753|        1|\n",
      "|         10|   Redford|  Roberti|                null|2008-07-21|        Clothing|     M| 72225|        7|\n",
      "+-----------+----------+---------+--------------------+----------+----------------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDF = spark.read.format(\"jdbc\").option(\"url\",url)\\\n",
    "    .option(\"dbTable\", \"employees\")\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"abc123\")\\\n",
    "    .option(\"driver\",driver).load()\n",
    "\n",
    "dbDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Table or view 'emp_from_jupyter' already exists. SaveMode: ErrorIfExists.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o53.save.\n: org.apache.spark.sql.AnalysisException: Table or view 'emp_from_jupyter' already exists. SaveMode: ErrorIfExists.;\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3e0c9989dcab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"postgres\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"abc123\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Table or view 'emp_from_jupyter' already exists. SaveMode: ErrorIfExists.;\""
     ]
    }
   ],
   "source": [
    "# writing to jdbc (postgreSQL DB)\n",
    "dbDF.where(\"salary > 150000\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"dbTable\", \"emp_from_jupyter\")\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"abc123\")\\\n",
    "    .option(\"driver\",driver).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query push-down - Spark pushes the query down to RDBMS DB to filter & load only required rows/columns. Here's demo\n",
    "\n",
    "dbDF.select(\"employee_id\", \"first_name\", \"salary\").where(\"salary > 160000\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"abc123\")\\\n",
    "    .option(\"driver\", driver)\\\n",
    "    .option(\"dbTable\", \"departments\").load()\n",
    "\n",
    "deptDF.select(\"*\").where(\"department like 'C%'\").distinct().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.where(\"department like 'C%'\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushed down query - In this case we will use custom query instead of a table name to fetch rows from source table\n",
    "\n",
    "pushedQuery = \"\"\"(select * from departments where division in ('Home', 'Electronics')) as dept\"\"\"\n",
    "\n",
    "deptDF = spark.read.format(\"jdbc\")\\\n",
    "            .option(\"url\", url)\\\n",
    "            .option(\"dbTable\", pushedQuery)\\\n",
    "            .option(\"user\", \"postgres\")\\\n",
    "            .option(\"password\", \"abc123\")\\\n",
    "            .option(\"driver\", driver).load()\n",
    "deptDF.show()\n",
    "\n",
    "deptDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can read in parallel from RDBMS tables\n",
    "\n",
    "deptDF = spark.read.format(\"jdbc\")\\\n",
    "            .option(\"url\", url)\\\n",
    "            .option(\"dbTable\", \"employees\")\\\n",
    "            .option(\"user\", \"postgres\")\\\n",
    "            .option(\"password\", \"abc123\")\\\n",
    "            .option(\"numPartitions\", 5)\\\n",
    "            .option(\"driver\", driver).load()\n",
    "\n",
    "deptDF.show()\n",
    "deptDF.explain()\n",
    "deptDF.rdd.getNumPartitions() # as data size is small, partition is still 1 but in large tables, it wont be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+--------------------+----------+----------+------+------+---------+\n",
      "|employee_id|  first_name|     last_name|               email| hire_date|department|gender|salary|region_id|\n",
      "+-----------+------------+--------------+--------------------+----------+----------+------+------+---------+\n",
      "|         11|      Nickey|       Pointon|npointona@vistapr...|2006-12-30|   Jewelry|     M|126333|        7|\n",
      "|         17|      Annora|      Bendelow|abendelowg@google...|2009-06-12|      Toys|     F| 75283|        5|\n",
      "|         18|      Ronica|      Armfield|                null|2012-09-14|      Toys|     F|114983|        5|\n",
      "|         42|    Penelopa|       Danieli|pdanieli15@micros...|2014-06-23|      Toys|     F| 85218|        1|\n",
      "|         64|         Ody|         Dunks|odunks1r@theguard...|2006-10-13|      Toys|     M| 92003|        1|\n",
      "|         66|         Deb|      Scarratt|                null|2010-12-24|   Jewelry|     F|112297|        3|\n",
      "|         68|  Wilhelmina|     Martignon|                null|2013-04-11|   Jewelry|     F|150818|        3|\n",
      "|         69|        Ivar|        Burwin|iburwin1w@hostgat...|2008-07-04|      Toys|     M| 67447|        4|\n",
      "|         72|     Zebulon|        Guppey|zguppey1z@virgini...|2004-11-30|      Toys|     M| 89221|        7|\n",
      "|         99|      Simeon|   Scarsbrooke|sscarsbrooke2q@te...|2011-04-10|   Jewelry|     M| 56976|        2|\n",
      "|        109|   Sigismund|Von Hindenburg|svonhindenburg30@...|2007-01-28|      Toys|     M|125291|        1|\n",
      "|        127|       Colby|       Soulsby|csoulsby3i@google...|2011-06-22|      Toys|     M|104665|        5|\n",
      "|        128|   Archibald|         Pavey|                null|2015-04-27|      Toys|     M| 59860|        1|\n",
      "|        151|Maximilianus|       Woonton| mwoonton46@ning.com|2008-12-18|      Toys|     M| 90399|        5|\n",
      "|        163|    Stirling|    MacSweeney|smacsweeney4i@sou...|2008-11-10|   Jewelry|     M| 37157|        5|\n",
      "|        182|      Jaimie|      Paulusch|jpaulusch51@sakur...|2004-10-10|   Jewelry|     F| 36979|        5|\n",
      "|        194|     Yardley|       Vynarde|                null|2003-02-28|   Jewelry|     M| 26876|        1|\n",
      "|        214|         Nap|    Corradetti|                null|2016-07-01|   Jewelry|     M|144262|        6|\n",
      "|        216|        Abby|     Seawright|aseawright5z@ask.com|2011-09-01|   Jewelry|     F| 56889|        3|\n",
      "|        217|      Sileas|       Rushman|srushman60@dion.n...|2011-02-05|      Toys|     F|118649|        6|\n",
      "+-----------+------------+--------------+--------------------+----------+----------+------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(employees) [numPartitions=2] [employee_id#64,first_name#65,last_name#66,email#67,hire_date#68,department#69,gender#70,salary#71,region_id#72] PushedFilters: [], ReadSchema: struct<employee_id:int,first_name:string,last_name:string,email:string,hire_date:date,department:...\n"
     ]
    }
   ],
   "source": [
    "# if we want to control which rows goes into which partition, then we can use something called 'predicates'. \n",
    "# at connection time, we can load the data into individual partitions into spark. Here's demo\n",
    "\n",
    "props = {\n",
    "    \"driver\":\"org.postgresql.Driver\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"abc123\"\n",
    "}\n",
    "predicates = [\n",
    "    \"department = 'Jewelry' or department = 'Toys'\",\n",
    "    \"salary between 50000 and 80000\"\n",
    "]\n",
    "table=\"employees\"\n",
    "\n",
    "empDF = spark.read.jdbc(url, table, predicates=predicates, properties=props)\n",
    "empDF.show()\n",
    "empDF.rdd.getNumPartitions()\n",
    "empDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+----------+-----------------+------+------+---------+\n",
      "|employee_id|first_name|last_name|               email| hire_date|       department|gender|salary|region_id|\n",
      "+-----------+----------+---------+--------------------+----------+-----------------+------+------+---------+\n",
      "|          7|    Ardeen|  Curwood|  acurwood6@1und1.de|2006-02-19|         Clothing|     F| 28995|        7|\n",
      "|         13|    Anetta|    Arnao|                null|2009-05-23|            Games|     F| 38162|        1|\n",
      "|         44|     Moise|   Turpey|   mturpey17@mac.com|2008-04-15|    Device Repair|     M| 33781|        1|\n",
      "|         46|     Lanny|  Comolli|lcomolli19@poster...|2014-06-19| Phones & Tablets|     F| 34250|        6|\n",
      "|         53|  Alphonse|    Shedd|ashedd1g@illinois...|2014-08-17|        Cosmetics|     M| 33272|        4|\n",
      "|         57|    Reuben|  Hallard|                null|2009-03-14|        First Aid|     M| 34572|        5|\n",
      "|         67|      Wain|  Betonia|wbetonia1u@prince...|2005-07-23|            Tools|     M| 26578|        3|\n",
      "|         83|    Elnora|    Babin|ebabin2a@foxnews.com|2003-02-22|        First Aid|     F| 34355|        5|\n",
      "|        113|   Myrtice|   Emmens|                null|2011-11-21|           Movies|     F| 36919|        6|\n",
      "|        115|    Peyter|   Jansey|pjansey36@linkedi...|2005-12-02|Children Clothing|     M| 23348|        4|\n",
      "|        124|      Remy|  Sperski|rsperski3f@vistap...|2004-06-19|          Grocery|     F| 22796|        1|\n",
      "|        125|    Valera| Southway|vsouthway3g@times...|2010-05-20|    Device Repair|     F| 25114|        6|\n",
      "|        134|    Demott|   Mouton|                null|2007-09-26|        Cosmetics|     M| 22284|        5|\n",
      "|        135|     Salli|  Caville|scaville3q@miibei...|2008-10-17|        Furniture|     F| 29124|        5|\n",
      "|        140|   Eugenia| Scourgie|escourgie3v@imdb.com|2011-06-29|          Camping|     F| 26747|        5|\n",
      "|        143|      Dasi|Kennefick|dkennefick3y@appl...|2006-04-30|    Device Repair|     F| 29158|        4|\n",
      "|        152|  Johnette| Claworth|jclaworth47@googl...|2009-12-13|            Games|     F| 27750|        3|\n",
      "|        154|  Sapphira| Stouther|sstouther49@photo...|2007-12-10|    Device Repair|     F| 39966|        3|\n",
      "|        156|    Joleen| Rothwell|jrothwell4b@fastc...|2004-10-24|           Sports|     F| 29838|        2|\n",
      "|        157|    Paulie|    Wadly|pwadly4c@nytimes.com|2015-04-09|         Pharmacy|     M| 37829|        5|\n",
      "+-----------+----------+---------+--------------------+----------+-----------------+------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(employees) [numPartitions=10] [employee_id#110,first_name#111,last_name#112,email#113,hire_date#114,department#115,gender#116,salary#117,region_id#118] PushedFilters: [], ReadSchema: struct<employee_id:int,first_name:string,last_name:string,email:string,hire_date:date,department:...\n"
     ]
    }
   ],
   "source": [
    "# partitioning based on sliding window\n",
    "# dividing records based on salary\n",
    "colName = \"salary\"\n",
    "lowerBound = 0\n",
    "upperBound = 200000\n",
    "numPartitions = 10\n",
    "\n",
    "empDF = spark.read.jdbc(url, table, column=colName, properties=props, \\\n",
    "                        lowerBound=lowerBound, upperBound=upperBound,numPartitions=numPartitions)\n",
    "\n",
    "empDF.show()\n",
    "empDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x111f39c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot employee salary distribution\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#empDF.toPandas().plot.hexbin(x='region_id',y='salary',gridsize=15)\n",
    "empDF.toPandas().plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'textFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2fca2bce25de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading text files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/by-day/2010-12-01.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'textFile'"
     ]
    }
   ],
   "source": [
    "# reading text files\n",
    "spark.read.textFile(\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/by-day/2010-12-01.csv\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD - Low level APIs\n",
    "There are two sets of low-level APIs: there is one for manipulating distributed data (RDDs), and another for distributing and manipulating distributed shared variables (broadcast variables and accumulators).\n",
    "\n",
    "### You should generally use the lower-level APIs in three situations:\n",
    "\n",
    "1. You need some functionality that you cannot find in the higher-level APIs; for example, if you need very tight control over physical data placement across the cluster.\n",
    "\n",
    "2. You need to maintain some legacy codebase written using RDDs.\n",
    "\n",
    "3. You need to do some custom shared variable manipulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RDD\n",
    "An RDD represents an immutable, partitioned collection of records that can be operated on in parallel. Unlike DataFrames though, where each record is a structured row containing fields with a known schema, in RDDs the records are just Java, Scala, or Python objects of the programmer’s choosing.\n",
    "\n",
    "RDDs give you complete control because every record in an RDD is a just a Java or Python object. You can store anything you want in these objects, in any format you want. This gives you great power, but not without potential issues. Every manipulation and interaction between values must be defined by hand, meaning that you must “reinvent the wheel” for whatever task you are trying to carry out. Also, optimizations are going to require much more manual work, because Spark does not understand the inner structure of your records as it does with the Structured APIs. For instance, Spark’s Structured APIs automatically store data in an optimzied, compressed binary format, so to achieve the same space-efficiency and performance, you’d also need to implement this type of format inside your objects and all the low-level operations to compute over it. Likewise, optimizations like reordering filters and aggregations that occur automatically in Spark SQL need to be implemented by hand. For this reason and others, we highly recommend using the Spark Structured APIs when possible.\n",
    "\n",
    "##### The Partitioner is probably one of the core reasons why you might want to use RDDs in your code. Specifying your own custom Partitioner can give you significant performance and stability improvements if you use it correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create RDD\n",
    "spark.range(10).toDF(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[142] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create RDD from a local collection, use parallelize method\n",
    "data = \"This is a big string with words of type string\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(data,2)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'myWords'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name the RDD so it shows in spark UI\n",
    "words.setName(\"myWords\")\n",
    "words.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/by-day/2010-12-01.csv MapPartitionsRDD[144] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create RDD from a data source\n",
    "myrdd = spark.sparkContext.textFile(\"file:///Users/Neeraj/Documents/Proj/Spark/data/retail-data/by-day/2010-12-01.csv\")\n",
    "myrdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country',\n",
       " '536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,2010-12-01 08:26:00,2.55,17850.0,United Kingdom',\n",
       " '536365,71053,WHITE METAL LANTERN,6,2010-12-01 08:26:00,3.39,17850.0,United Kingdom',\n",
       " '536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,2010-12-01 08:26:00,2.75,17850.0,United Kingdom']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read records from rdd\n",
    "myrdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct words\n",
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'big', 'with', 'words', 'of', 'type']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering using functions\n",
    "def myStartWith(word):\n",
    "    return word.startswith(\"s\")\n",
    "\n",
    "print(startWithS(\"super\"))\n",
    "\n",
    "words.filter(lambda word: not myStartWith(word)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'T', False),\n",
       " ('is', 'i', False),\n",
       " ('a', 'a', False),\n",
       " ('big', 'b', False),\n",
       " ('string', 's', True),\n",
       " ('with', 'w', False),\n",
       " ('words', 'w', False),\n",
       " ('of', 'o', False),\n",
       " ('type', 't', False),\n",
       " ('string', 's', True)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using map\n",
    "w = words.map(lambda word: (word, word[0], myStartWith(word)))\n",
    "w.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('string', 's', True), ('string', 's', True)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.filter(lambda x: x[2]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'h', 'i', 's', 'i', 's', 'a', 'b', 'i', 'g']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "words.flatMap(lambda w: list(w)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['string', 'string', 'words', 'This', 'with', 'type', 'big', 'is', 'of', 'a']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort in descending or reverse order\n",
    "words.sortBy(lambda s: len(s) * -1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  ['a', 'with', 'of', 'type']\n",
      "2:  ['This', 'is', 'big', 'string', 'words', 'string']\n"
     ]
    }
   ],
   "source": [
    "# splitting a RDD\n",
    "splitted = words.randomSplit([0.3, 0.7])\n",
    "print(\"1: \",splitted[0].collect())\n",
    "print(\"2: \",splitted[1].collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of all numbers in a RDD\n",
    "spark.sparkContext.parallelize(range(1,21)).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get longest word in data rdd\n",
    "def getLongestWord(l, r):\n",
    "    if len(l) > len(r):\n",
    "        return l\n",
    "    else:\n",
    "        return r\n",
    "    \n",
    "words.reduce(getLongestWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 1,\n",
       "             'is': 1,\n",
       "             'a': 1,\n",
       "             'big': 1,\n",
       "             'string': 2,\n",
       "             'with': 1,\n",
       "             'words': 1,\n",
       "             'of': 1,\n",
       "             'type': 1})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByValue - loads entire data into driver's memory and then count value occurences. Avoid it for large datasets\n",
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'big', 'string']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take - gets values from one partition and then based on how many values are remaining, it fetches from next partition\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'a', 'big', 'is', 'of']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['words', 'with', 'type', 'string', 'string']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'with', 'words', 'This', 'string']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.takeSample(withReplacement=False, num=5, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving RDDs in files\n",
    "words.saveAsTextFile(\"file:///Users/Neeraj/Downloads/words\")\n",
    "words.map(lambda w: (w, len(w))).saveAsSequenceFile(\"file:///Users/Neeraj/Downloads/words1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache a RDD in memory\n",
    "words.cache()\n",
    "words.collect()\n",
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'big', 'string', 'with', 'words', 'of', 'type', 'string']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint - this feature isn't available in dataset or dataframes. It allows to store a RDD on disk instead of memory\n",
    "# so it can be accessed later without recomputing.\n",
    "spark.sparkContext.setCheckpointDir(\"file:///Users/Neeraj/Downloads/checkpoint\")\n",
    "words.checkpoint()\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['       5', '       5']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pipe to invoke external processes. This is great method to invoke existing programs\n",
    "words.pipe('wc -l').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Running shell script',\n",
       " 'THIS',\n",
       " 'IS',\n",
       " 'A',\n",
       " 'BIG',\n",
       " 'STRING',\n",
       " 'Running shell script',\n",
       " 'WITH',\n",
       " 'WORDS',\n",
       " 'OF',\n",
       " 'TYPE',\n",
       " 'STRING']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is invoked once per partition as seen in the results\n",
    "words.pipe('./upperCase.sh').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xml data\n",
    "# start pyspark as \n",
    "# pyspark --packages com.databricks:spark-xml_2.10:0.4.1\n",
    "\n",
    "#from os import environ\n",
    "#environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'\n",
    "\n",
    "xmlDF = spark.read.format(\"xml\")\\\n",
    "        .options(rowTag=\"book\")\\\n",
    "        .load(\"file:///Users/Neeraj/Documents/Proj/Spark/data/books.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|  _id|              author|         description|          genre|price|publish_date|               title|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|bk101|Gambardella, Matthew|An in-depth look ...|       Computer|44.95|  2000-10-01|XML Developer's G...|\n",
      "|bk102|          Ralls, Kim|A former architec...|        Fantasy| 5.95|  2000-12-16|       Midnight Rain|\n",
      "|bk103|         Corets, Eva|After the collaps...|        Fantasy| 5.95|  2000-11-17|     Maeve Ascendant|\n",
      "|bk104|         Corets, Eva|In post-apocalyps...|        Fantasy| 5.95|  2001-03-10|     Oberon's Legacy|\n",
      "|bk105|         Corets, Eva|The two daughters...|        Fantasy| 5.95|  2001-09-10|  The Sundered Grail|\n",
      "|bk106|    Randall, Cynthia|When Carla meets ...|        Romance| 4.95|  2000-09-02|         Lover Birds|\n",
      "|bk107|      Thurman, Paula|A deep sea diver ...|        Romance| 4.95|  2000-11-02|       Splish Splash|\n",
      "|bk108|       Knorr, Stefan|An anthology of h...|         Horror| 4.95|  2000-12-06|     Creepy Crawlies|\n",
      "|bk109|        Kress, Peter|After an inadvert...|Science Fiction| 6.95|  2000-11-02|        Paradox Lost|\n",
      "|bk110|        O'Brien, Tim|Microsoft's .NET ...|       Computer|36.95|  2000-12-09|Microsoft .NET: T...|\n",
      "|bk111|        O'Brien, Tim|The Microsoft MSX...|       Computer|36.95|  2000-12-01|MSXML3: A Compreh...|\n",
      "|bk112|         Galos, Mike|Microsoft Visual ...|       Computer|49.95|  2001-04-16|Visual Studio 7: ...|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xmlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
